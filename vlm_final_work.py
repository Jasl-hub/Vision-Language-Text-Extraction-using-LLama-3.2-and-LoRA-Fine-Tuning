# -*- coding: utf-8 -*-
"""VLM_Final-work.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17wNMY_KOHTClFBbgmrMod2DBnZ-dc-zy
"""

!pip install pytesseract pillow

!sudo apt update
!sudo apt install tesseract-ocr

!pip install datasets

!pip install torch transformers datasets huggingface_hub

!huggingface-cli login

from google.colab import drive
drive.mount('/content/drive')

!pip install compressed-tensors

import pytesseract
import json
from PIL import Image
import os
import torch
import transformers
from transformers import AutoProcessor, AutoModelForVision2Seq
from datasets import load_dataset, Dataset
import random
from PIL import Image
import os
from sklearn.model_selection import train_test_split

!unzip '/content/drive/MyDrive/VLM.zip' -d '/content/images'

# Load dataset (Assuming images are stored in 'data/images/' directory)
image_folder = "/content/images/images"
image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(('.png', '.jpg', '.jpeg'))]

"""PHASE 2 : GENERATE GROUND TEXT -  Tesseract OCR with PSM 1, OEM 3"""

# Ensure Tesseract is installed and set the correct path
# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"

# Directory containing the images
image_dir = "/content/images/images"  # Replace with your folder path
image_files = sorted([f for f in os.listdir(image_dir) if f.endswith((".jpg", ".png", ".jpeg"))])[:50]

def extract_text_tesseract(image_path):
    """Extract text from an image using Tesseract OCR with PSM 1 for multi-column detection."""
    image = Image.open(image_path)
    custom_config = r'--psm 1 --oem 3'  # Use Layout Analysis (for multi-column text)
    extracted_text = pytesseract.image_to_string(image, config=custom_config)
    return extracted_text


# Loop through 124 images and extract text
ocr_results = {}

# Process images
for image_file in image_files:
    image_path = os.path.join(image_dir, image_file)
    extracted_text = extract_text_tesseract(image_path)
    ocr_results[image_file] = extracted_text
    print(f"Processed: {image_file}")

# Save results in JSON format
json_output_path = "/content/ocr_results.json"
with open(json_output_path, "w", encoding="utf-8") as f:
    json.dump(ocr_results, f, indent=4, ensure_ascii=False)

print(f"\nâœ… OCR processing complete. Results saved in '{json_output_path}'.")

!pip install jiwer

!pip install symspellpy

dictionary_content = """the 23135851162
be 1254582419
to 1241571399
of 1162216295
and 1074107302
a 1014422422
in 699643722
that 592605055
have 486899760
I 444131760"""

# Save the dictionary to a file
with open("frequency_dictionary_en_82_765.txt", "w") as f:
    f.write(dictionary_content)

from symspellpy import SymSpell, Verbosity

# Initialize SymSpell
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# Load the generated dictionary file
dictionary_path = "/content/frequency_dictionary_en_82_765.txt"
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

# Load JSON file
with open("/content/ocr_results.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Convert dictionary to lists
X = list(data.keys())  # Image file names
y = list(data.values())  # Extracted text

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

print(f"Training Samples: {len(X_train)}, Testing Samples: {len(X_test)}")

"""PHASE 3: RUN PRE-TRAINED MODEL & EVALUATE ON TEST IMAGES (USING WER, CER)"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load LLaMA 3.2 (11B) model
model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name,  padding_side='left')
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

from jiwer import wer, cer

# Load OCR dataset
with open("/content/ocr_results.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Train-Test Split
X = list(data.keys())  # Image file names
y = list(data.values())  # Extracted OCR text

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

print(f"Training Samples: {len(X_train)}, Testing Samples: {len(X_test)}")

batch_size = 4  # Adjust based on available memory
num_samples = len(X_test)

# Run LLaMA on `X_test`
predictions = {}

def correct_ocr_with_symspell(text):
    suggestions = sym_spell.lookup(text, Verbosity.ALL, 2)  # Correct argument order
    return suggestions[0].term if suggestions else text  # Return corrected word


for i in range(0, num_samples, batch_size):
    batch_X = X_test[i : i + batch_size]  # Select batch of image names
    batch_y = y_test[i : i + batch_size]  # Select batch of OCR text

    # âœ… Apply OCR correction using SymSpell before LLaMA inference
    # batch_inputs = [f"Fix OCR mistakes and return clean text:\n\n{correct_ocr_with_symspell(text)}" for text in batch_y]
    batch_inputs = [f"Fix OCR mistakes and return clean text:\n\n{correct_ocr_with_symspell(text)}" for text in batch_y]

    # âœ… Tokenize and move to GPU
    inputs = tokenizer(batch_inputs, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cuda")

    with torch.no_grad():
        batch_outputs = model.generate(**inputs, max_new_tokens=100)

    # âœ… Decode batch outputs
    batch_generated_texts = tokenizer.batch_decode(batch_outputs, skip_special_tokens=True)

    # âœ… Store results
    for image_name, generated_text in zip(batch_X, batch_generated_texts):
        predictions[image_name] = generated_text
        print(f"Processed: {image_name}")

# Save predictions to JSON
with open("ocr_predictions.json", "w", encoding="utf-8") as f:
    json.dump(predictions, f, indent=4)

print("\nâœ… LLaMA 3.2 Inference Complete! Predictions saved to 'ocr_predictions.json'.")

# Calculate WER & CER
total_wer, total_cer = 0, 0
sample_count = len(predictions)

for image_name in predictions.keys():
    original_text = y_test[X_test.index(image_name)]  # OCR extracted text (ground truth)
    cleaned_text = predictions[image_name]  # LLaMA generated text

    total_wer += wer(original_text, cleaned_text)
    total_cer += cer(original_text, cleaned_text)
    print(total_wer)
    print(total_cer)

avg_wer = total_wer / sample_count
avg_cer = total_cer / sample_count

print(f"\nðŸ“Š Evaluation Results:")
print(f"ðŸ”¹ Average Word Error Rate (WER): {avg_wer:.4f}")
print(f"ðŸ”¹ Average Character Error Rate (CER): {avg_cer:.4f}")

"""PHASE 4 : Refine Extracted Text"""

!pip install nltk spacy
!python -m spacy download en_core_web_sm

import json
import nltk
nltk.download('punkt_tab')
import spacy
from nltk.tokenize import sent_tokenize

# Load NLP Models
nltk.download("punkt")
nlp = spacy.load("en_core_web_sm")

def refine_text(text):
    """Cleans and refines OCR-extracted text."""
    doc = nlp(text)

    # Convert to lowercase and remove extra spaces
    cleaned_text = " ".join([token.text.lower() for token in doc if not token.is_space])

    # Sentence tokenization (helps readability)
    sentences = sent_tokenize(cleaned_text)
    cleaned_text = " ".join(sentences)

    return cleaned_text

# âœ… Load extracted text from JSON
with open("/content/ocr_results.json", "r", encoding="utf-8") as f:
    extracted_data = json.load(f)

# âœ… Apply text refinement
cleaned_data = {img: refine_text(text) for img, text in extracted_data.items()}

# âœ… Save cleaned dataset
with open("/content/ocr_results_cleaned.json", "w", encoding="utf-8") as f:
    json.dump(cleaned_data, f, indent=4)

print("\nâœ… OCR text cleaned & saved to 'ocr_results_cleaned.json'.")

"""PHASE 5: Format Data for Fine-Tuning &  Fine-Tune LLaMA 3.2 Vision Model Using LoRA"""

# âœ… Convert JSON into a structured dataset
train_data = [{"image": img, "ocr_text": text} for img, text in cleaned_data.items()]

# âœ… Save structured training data
with open("/content/ocr_results_cleaned.json", "w", encoding="utf-8") as f:
    json.dump(train_data, f, indent=4)

print("\nâœ… Training dataset prepared for LLaMA fine-tuning.")

!pip install peft transformers datasets accelerate bitsandbytes

!pip install datasets peft transformers accelerate

# !git clone https://github.com/TimDettmers/bitsandbytes.git
# %cd bitsandbytes
# CUDA_VERSION=124
# !python setup.py install

# !huggingface-cli download meta-llama/Llama-3.2-11B-Vision-Instruct --local-dir ./llama3_model

# !pip install bitsandbytes==0.41.1

# !pip uninstall -y bitsandbytes
# !rm -rf /usr/local/lib/python3.11/dist-packages/bitsandbytes*
# !rm -rf /usr/local/lib/python3.11/site-packages/bitsandbytes*

!pip uninstall -y bitsandbytes
!pip install bitsandbytes==0.44.0

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import get_peft_model, LoraConfig, TaskType
from transformers import Trainer, DataCollatorForSeq2Seq


model_name = "meta-llama/Llama-3.2-11B-Vision-Instruct"

# âœ… Properly Define QLoRA 4-bit Quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16  # âœ… Keep inside BitsAndBytesConfig
)

# âœ… Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… Load Model with Correct Configuration
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,  # âœ… Correct usage
    torch_dtype=torch.float16,  # âœ… Ensures model weights load correctly
    device_map="auto"  # âœ… Automatically maps model to GPU or CPU
)

!nvidia-smi

# import torch
# import bitsandbytes as bnb
# print(torch.__version__)
# !pip show bitsandbytes
# print("âœ… bitsandbytes version:", bnb.__version__)
# # both should be same versions

import torch
torch.cuda.empty_cache()
#empty cache memory b4 training

from datasets import load_dataset  # âœ… Import the missing function


# âœ… Load Training Dataset
dataset = load_dataset("json", data_files={"train": "/content/ocr_results_cleaned.json"})["train"]
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from transformers import Trainer, DataCollatorForSeq2Seq

# âœ… Apply LoRA Configuration
lora_config = LoraConfig(
    r=8,  # LoRA Rank
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # Apply LoRA to key transformer layers
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)

model = get_peft_model(model, lora_config)

# âœ… Training Arguments
training_args = TrainingArguments(
    output_dir="./llama_ocr_finetuned",
    per_device_train_batch_size=1,  # Reduce if OOM
    gradient_accumulation_steps=8,  # Simulates larger batch size
    optim="adamw_bnb_8bit",  # âœ… Use 8-bit optimizer for lower memory usage
    evaluation_strategy="no",
    save_strategy="epoch",
    save_total_limit=2,
    num_train_epochs=3,  # Adjust for better results
    learning_rate=5e-5,
    weight_decay=0.01,
    fp16=True,  # Use mixed precision for faster training
    logging_steps=10,
    report_to="none",  # Disable WandB logging
    remove_unused_columns=False
)

print(model)

def preprocess_function(example):
    """
    Tokenize OCR text and prepare inputs for fine-tuning.
    """
    encoding = tokenizer(example["ocr_text"], truncation=True, padding="max_length", max_length=512)

    # âœ… Shift input_ids to create labels for Causal LM
    encoding["labels"] = encoding["input_ids"].copy()

    return encoding

# âœ… Apply Tokenization with Labels
tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["image", "ocr_text"])

# âœ… Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,  # âœ… Use tokenized dataset
    data_collator=DataCollatorForSeq2Seq(tokenizer),
)

# âœ… Fine-Tune Model
trainer.train()

from sklearn.model_selection import train_test_split
import datasets

# âœ… Load OCR dataset
dataset = datasets.load_dataset("json", data_files="/content/ocr_results_cleaned.json")["train"]

# âœ… Convert to Pandas for easy splitting
dataset_df = dataset.to_pandas()

# âœ… Split: 80% train, 20% test
_, test_df = train_test_split(dataset_df, test_size=0.2, random_state=42)

# âœ… Convert back to Hugging Face dataset
test_dataset = datasets.Dataset.from_pandas(test_df)

# âœ… Print dataset size
print(f"Test Samples: {len(test_dataset)}")

"""Ground truth should come from Phase 1 to run Test samples"""

# âœ… Load Ground Truth Data from JSON
with open("/content/ocr_results_cleaned.json", "r", encoding="utf-8") as f:
    ground_truth_data = json.load(f)  # âœ… Now it's a dictionary

# Print the first few samples to check the structure
# print(json.dumps(data[:5], indent=4))

# expected format of ground truth(human-annotated)
# [
#     {
#         "image": "image_001.jpg",
#         "ocr_text": "Ths is an exmple txt.",
#         "ground_truth": "This is an example text."
#     },
#     {
#         "image": "image_002.jpg",
#         "ocr_text": "Welcom to AI model.",
#         "ground_truth": "Welcome to the AI model."
#     }
# ]

# !pip install jiwer

import json
import torch
from jiwer import wer, cer

# âœ… Load Ground Truth Data & Convert to Dictionary
with open("/content/ocr_results_cleaned.json", "r", encoding="utf-8") as f:
    ground_truth_list = json.load(f)

# âœ… Convert List to Dictionary
ground_truth_data = {entry["image"]: entry["ocr_text"] for entry in ground_truth_list}

def evaluate_ocr_quality(model, tokenizer, test_dataset, ground_truth_data):
    predictions = []
    references = []

    for sample in test_dataset:
        image_name = sample["image"]  # Get image name
        input_text = sample["ocr_text"]  # OCR output before fine-tuning

        # âœ… Get ground truth text (manually corrected if available)
        reference_text = ground_truth_data.get(image_name, input_text)  # Use OCR as fallback if GT is missing

        inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

        with torch.no_grad():
            output = model.generate(**inputs, max_new_tokens=100)

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

        predictions.append(generated_text)
        references.append(reference_text)

    # âœ… Compute WER & CER
    avg_wer = wer(references, predictions)
    avg_cer = cer(references, predictions)

    print(f"\nðŸ“Š Evaluation Results:\nðŸ”¹ WER: {avg_wer:.4f}\nðŸ”¹ CER: {avg_cer:.4f}")

# âœ… Run Evaluation
evaluate_ocr_quality(model, tokenizer, test_dataset, ground_truth_data)

